# 卷积神经网络

## 1. alexnet

## 2. Lenet

## 3. VGG net

## 4. GoogleNet

## 5. ResNet

## 前置知识

### 1. 1*1卷积核

直观的例子: RGBA -> Grey(4通道->1通道), 无激活函数

#### 原理
1×1卷积核本质上就是一个1×1大小的滤波器，它在输入特征图上进行滑动，通过与输入特征图对应位置的元素进行点积运算，然后求和得到输出特征图的一个值。虽然1×1卷积核看起来似乎不能提取空间特征，但它可以在不改变特征图空间尺寸（高度和宽度）的情况下，对通道数进行调整，还可以实现特征的线性组合，增加模型的非线性表达能力。

#### 计算步骤
假设输入特征图的形状为 $[H, W, C_{in}]$，其中 $H$ 是高度，$W$ 是宽度，$C_{in}$ 是输入通道数；有 $C_{out}$ 个1×1卷积核，每个卷积核的形状为 $[1, 1, C_{in}]$，输出特征图的形状为 $[H, W, C_{out}]$。计算步骤如下：
1. **初始化输出特征图**：创建一个形状为 $[H, W, C_{out}]$ 的全零张量，用于存储卷积运算的结果。
2. **遍历每个卷积核**：对于每个卷积核，在输入特征图上进行滑动操作。
3. **点积与求和**：在每个位置上，将卷积核与输入特征图对应位置的元素进行点积运算，然后将结果求和，得到输出特征图对应位置的值。
4. **重复步骤2和3**：直到所有卷积核都处理完毕。

#### 1x1卷积核在卷积神经网络中的应用
1×1卷积核在深度学习领域有着广泛的应用，下面详细介绍它在不同场景下的使用方式和作用：

##### 减少或增加通道数
- **作用**：1×1卷积核可以在不改变特征图的高度和宽度的情况下，对通道数进行灵活调整。当需要减少通道数时，可以降低模型的计算量和参数数量，起到降维的作用；当需要增加通道数时，可以丰富特征的表达能力。
- **应用场景**
    - **网络压缩**：在一些大型的卷积神经网络中，中间层的特征图通道数可能非常多，这会导致计算量和存储需求急剧增加。通过使用1×1卷积核进行降维，可以减少后续层的计算负担，提高模型的运行效率。例如，在GoogLeNet中，就大量使用了1×1卷积核来减少通道数，从而降低模型的复杂度。
    - **特征融合**：在某些情况下，需要将不同通道的特征进行融合，以提取更丰富的信息。可以使用1×1卷积核增加通道数，使得模型能够学习到更多的特征组合。

##### 增加模型的非线性表达能力
- **作用**：在卷积神经网络中，通常会在卷积层之后添加激活函数（如ReLU），以引入非线性因素。1×1卷积核与激活函数的组合可以增加模型的非线性表达能力，使得模型能够学习到更复杂的特征。
- **应用场景**
    - **图像分类**：在图像分类任务中，需要模型能够学习到图像的各种特征，以区分不同的类别。通过在网络中插入多个1×1卷积层和激活函数，可以增强模型的非线性表达能力，提高分类的准确率。
    - **目标检测**：目标检测任务需要模型能够准确地定位和识别图像中的目标。1×1卷积核可以帮助模型学习到更复杂的特征，从而提高目标检测的性能。

##### 跨通道信息交互
- **作用**：1×1卷积核可以对输入特征图的每个位置的不同通道进行线性组合，实现跨通道的信息交互。这种交互可以帮助模型更好地捕捉不同通道之间的相关性，从而提取更有效的特征。
- **应用场景**
    - **语义分割**：语义分割任务需要对图像中的每个像素进行分类，因此需要模型能够充分利用不同通道的信息。1×1卷积核可以促进跨通道的信息交流，使得模型能够更好地理解图像的语义信息，提高分割的精度。
    - **实例分割**：实例分割不仅要对像素进行分类，还要区分不同的实例。通过1×1卷积核进行跨通道信息交互，可以帮助模型更好地捕捉实例之间的差异，提高实例分割的效果。

##### 替代全连接层
- **作用**：在传统的卷积神经网络中，全连接层通常用于将卷积层提取的特征映射到分类或回归的输出空间。然而，全连接层的参数数量较多，容易导致过拟合。1×1卷积核可以在一定程度上替代全连接层，减少参数数量，提高模型的泛化能力。
- **应用场景**
    - **移动端应用**：在移动端设备上，由于计算资源和存储容量有限，需要使用轻量级的模型。使用1×1卷积核替代全连接层可以减少模型的参数数量，降低计算复杂度，使得模型更适合在移动端运行。
    - **实时处理**：在一些需要实时处理的任务中，如实时视频分析，需要模型能够快速地进行推理。1×1卷积核的计算效率较高，可以提高模型的推理速度，满足实时处理的需求。

### 2. 最大池化下采样

最大池化下采样层（Max Pooling Layer）是卷积神经网络（CNN）中常用的一种**下采样操作**，主要用于降低特征图的空间尺寸（高度和宽度），**通道数不变**，同时保留关键特征。以下是其核心要点：


### **1. 定义与作用**
- **定义**：对输入特征图的每个**池化窗口**（如2×2或3×3区域）取最大值，作为输出特征图的对应位置值。
- **作用**：
  - **减少计算量**：降低特征图尺寸，减少后续层的参数数量和计算复杂度。
  - **防止过拟合**：通过降低空间分辨率，增强模型对平移、旋转等变换的鲁棒性。
  - **保留显著特征**：提取最突出的特征（如边缘、纹理），抑制背景噪声。


### **2. 工作原理**
- **输入**：形状为 `[H, W, C]` 的特征图（H=高度，W=宽度，C=通道数）。
- **池化窗口**：通常为2×2或3×3，步长（Stride）与窗口大小相同（默认不重叠）。
- **输出**：新特征图的尺寸为 `[H/2, W/2, C]`（假设窗口为2×2且步长=2）。
- **示例**：
  ```
  输入窗口：
  [[3, 1, 4],
   [2, 5, 7],
   [9, 6, 8]]
  池化窗口=2×2，步长=2 → 输出：[[5], [9]]
  ```


### **3. 关键参数**
- **池化窗口大小**：常用2×2或3×3。
- **步长（Stride）**：控制窗口移动的步幅，默认等于窗口大小（无重叠）。
- **填充（Padding）**：边缘补零以保持尺寸对齐（如池化窗口3×3，步长2，填充1）。


### **4. 与平均池化的对比**
| **操作** | **最大池化**             | **平均池化**                |
| -------- | ------------------------ | --------------------------- |
| 输出值   | 窗口内最大值             | 窗口内平均值                |
| 适用场景 | 保留边缘、纹理等显著特征 | 模糊背景，突出整体形状      |
| 常见使用 | 主流CNN（如VGG、ResNet） | 分类任务后期（如Inception） |


### **5. 典型应用场景**
- **图像分类**：在VGG、ResNet等网络中，通过多次池化逐步降低特征图尺寸。
- **目标检测**：YOLO、Faster R-CNN等模型利用池化提取多尺度特征。
- **语义分割**：U-Net等模型结合池化与上采样实现上下文信息聚合。


### **6. 注意事项**
- **不可学习性**：池化操作是固定规则，无需训练参数。
- **位置信息丢失**：池化后特征图的空间位置精度降低，可能影响定位任务（需结合反卷积或注意力机制补偿）。


### **代码示例（PyTorch）**
```python
import torch
import torch.nn as nn

# 定义最大池化层（窗口2×2，步长2）
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

# 输入特征图（形状：[1, 3, 224, 224]）
input = torch.randn(1, 3, 224, 224)
output = max_pool(input)  # 输出形状：[1, 3, 112, 112]
```


### **总结**
最大池化是CNN中高效的下采样手段，通过保留局部最大值浓缩特征，在减少计算量的同时提升模型的鲁棒性。其核心思想是“聚焦关键特征，忽略冗余细节”，是构建深度视觉模型的基础组件之一。





## pytorch相关

### Conv2d填充规则
在PyTorch中，不同的函数或模块对于填充（padding）的处理方式有所不同，下面分别介绍几种常见的情况：

#### 1. `torch.nn.Conv2d` 模块
在二维卷积层 `torch.nn.Conv2d` 中，当使用整数作为 `padding` 参数时，默认是在输入特征图的四周添加相同数量的填充。

**示例代码**：
```python
import torch
import torch.nn as nn

# 创建一个卷积层，设置padding为1
conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)

# 生成一个随机输入张量
input_tensor = torch.randn(1, 3, 32, 32)

# 进行卷积操作
output = conv(input_tensor)

# 打印输入和输出的尺寸
print(f"输入尺寸: {input_tensor.shape}")
print(f"输出尺寸: {output.shape}")
```
在上述代码中，`padding=1` 表示在输入特征图的上下左右四个方向都添加1个单位的填充。

如果想在不同的方向上设置不同的填充值，可以使用元组形式，例如 `padding=(1, 2)` 表示在高度方向上上下各填充1个单位，在宽度方向上左右各填充2个单位。

#### 2. `torch.nn.MaxPool2d` 模块
对于二维最大池化层 `torch.nn.MaxPool2d`，`padding` 参数的使用方式与 `Conv2d` 类似。当使用整数作为 `padding` 参数时，同样是在输入特征图的四周添加相同数量的填充。

**示例代码**：
```python
import torch
import torch.nn as nn

# 创建一个最大池化层，设置padding为1
pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)

# 生成一个随机输入张量
input_tensor = torch.randn(1, 3, 32, 32)

# 进行池化操作
output = pool(input_tensor)

# 打印输入和输出的尺寸
print(f"输入尺寸: {input_tensor.shape}")
print(f"输出尺寸: {output.shape}")
```
这里 `padding=1` 表示在输入特征图的上下左右四个方向都添加1个单位的填充。同样，也可以使用元组形式设置不同方向的填充值。

#### 3. `torch.nn.functional.pad` 函数
`torch.nn.functional.pad` 函数提供了更灵活的填充方式，它允许你指定填充的维度和填充值。默认情况下，它不是简单地在四周添加相同的填充，而是需要你明确指定填充的范围。

**示例代码**：
```python
import torch
import torch.nn.functional as F

# 生成一个随机输入张量
input_tensor = torch.randn(1, 3, 32, 32)

# 在高度和宽度方向上各填充1个单位
padded_tensor = F.pad(input_tensor, (1, 1, 1, 1))

# 打印输入和输出的尺寸
print(f"输入尺寸: {input_tensor.shape}")
print(f"输出尺寸: {padded_tensor.shape}")
```
在 `F.pad` 函数中，参数 `(1, 1, 1, 1)` 表示在宽度方向上右侧填充1个单位、左侧填充1个单位，在高度方向上底部填充1个单位、顶部填充1个单位。参数的顺序是从最后一个维度开始依次向前指定填充值。

综上所述，在 `Conv2d` 和 `MaxPool2d` 模块中，当使用整数作为 `padding` 参数时默认是在四周添加相同的填充；而 `torch.nn.functional.pad` 函数需要你明确指定填充的范围。 